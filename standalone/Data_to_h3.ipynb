{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "import requests\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import h3\n",
    "from shapely.geometry import Point, shape\n",
    "from multiprocessing import Pool\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_urls(base_url):\n",
    "    \"\"\"\n",
    "    This function scrapes all URLs from a webpage.\n",
    "\n",
    "    Args:\n",
    "        base_url (str): The URL of the webpage to scrape.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of URLs found on the webpage.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(base_url)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        urls = [a['href'] for a in soup.find_all('a', href=True) if 'cbs_vk100' in a['href']]\n",
    "        return urls\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to scrape {base_url} due to {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_gpkg(gpkg_path, shapefile_path, output_path, epsg_code=4326):\n",
    "    \"\"\"\n",
    "    This function clips a GeoPackage to the extent of a shapefile and saves the result as a GeoJSON.\n",
    "\n",
    "    Args:\n",
    "        gpkg_path (str): Path to the GeoPackage.\n",
    "        shapefile_path (str): Path to the shapefile.\n",
    "        output_path (str): Path to save the clipped GeoJSON.\n",
    "        epsg_code (int): EPSG code for coordinate system. Default is 4326 (WGS84).\n",
    "\n",
    "    \"\"\"\n",
    "    try:\n",
    "        geo_df = gpd.read_file(gpkg_path)\n",
    "        geo_df = geo_df.to_crs(epsg=epsg_code)\n",
    "        clip_geo_df = gpd.read_file(shapefile_path)\n",
    "        clip_geo_df = clip_geo_df.to_crs(epsg=epsg_code)\n",
    "        clipped = gpd.clip(geo_df, clip_geo_df)\n",
    "        clipped.to_file(output_path, driver='GeoJSON')\n",
    "    except Exception as e:\n",
    "        print(f\"Error clipping GeoPackage: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_extract(url, year, download_path, extract_path):\n",
    "    \"\"\"\n",
    "    This function downloads a ZIP file from a URL and extracts its contents.\n",
    "\n",
    "    Args:\n",
    "        url (str): The URL of the ZIP file.\n",
    "        year (int): The year of the data.\n",
    "        download_path (str): Path to save the downloaded ZIP file.\n",
    "        extract_path (str): Path to extract the contents of the ZIP file.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the file was successfully downloaded and extracted, False otherwise.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        with open(download_path, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "        with zipfile.ZipFile(download_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(extract_path)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading or extracting file: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_feature(feature, resolution):\n",
    "    \"\"\"\n",
    "    This function processes a single GeoDataFrame feature and converts geometry to H3 cell.\n",
    "\n",
    "    Args:\n",
    "        feature (tuple): A single row from the DataFrame, represented as a tuple.\n",
    "        resolution (int): H3 resolution (0-17).\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the H3 cell and population data.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        aantal_inwoners, geometry = feature\n",
    "        centroid = geometry.centroid\n",
    "        h3_cell = h3.geo_to_h3(centroid.y, centroid.x, resolution)\n",
    "        return h3_cell, aantal_inwoners\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing feature: {e}\")\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_geojson(clip_gdf, resolution, output_dir):\n",
    "    \"\"\"\n",
    "    This function processes a GeoJSON file containing population data and aggregates it to H3 cells.\n",
    "\n",
    "    Args:\n",
    "        clip_gdf (str): URL or path to the GeoJSON file.\n",
    "        resolution (int): H3 resolution (0-17).\n",
    "        output_dir (str): Directory to save the resulting CSV files.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        gdf = gpd.read_file(clip_gdf)\n",
    "        gdf = gdf.to_crs(epsg=4326)\n",
    "\n",
    "        n_cores = os.cpu_count()\n",
    "        with Pool(processes=n_cores) as pool:\n",
    "            for column in gdf.columns:\n",
    "                if column != 'geometry':\n",
    "                    # Check if column data is numeric\n",
    "                    if pd.api.types.is_numeric_dtype(gdf[column]):\n",
    "                        # Replace negative values with 0\n",
    "                        gdf[column] = gdf[column].clip(lower=0)\n",
    "\n",
    "\n",
    "                    data = list(gdf[[column, 'geometry']].itertuples(index=False, name=None))\n",
    "                    results = pool.starmap(process_feature, zip(data, [resolution] * len(gdf)))\n",
    "                    h3_data, population_data = zip(*results)\n",
    "                    df = pd.DataFrame({'hex9': h3_data, 'value': population_data})\n",
    "\n",
    "                    # Group by 'hex9' and sum 'value'\n",
    "                    df = df.groupby('hex9')['value'].sum().reset_index()\n",
    "\n",
    "                    output_filepath = os.path.join(output_dir, f'{column}_h3.csv')\n",
    "                    df.to_csv(output_filepath, index=False)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing data: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    This is the main function that orchestrates the execution of all other functions.\n",
    "    \"\"\"\n",
    "    current_year = datetime.datetime.now().year -1\n",
    "    base_url = 'https://www.cbs.nl/nl-nl/dossier/nederland-regionaal/geografische-data/kaart-van-100-meter-bij-100-meter-met-statistieken#:~:text=In%20deze%20kaart%20met%20vierkanten,en%20nabijheid%20van%20voorzieningen%20samengesteld.'\n",
    "    urls = scrape_urls(base_url)\n",
    "    for year in range(current_year - 1, current_year - 12, -1):\n",
    "        # Create separate directories for downloads, extracts, and outputs\n",
    "        download_dir = './downloads'\n",
    "        extract_dir = './extracts'\n",
    "        output_dir = './outputs'\n",
    "        os.makedirs(download_dir, exist_ok=True)\n",
    "        os.makedirs(extract_dir, exist_ok=True)\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        download_path = os.path.join(download_dir, f'cbs_{year}.zip')\n",
    "        extract_path = os.path.join(extract_dir, f'cbs_{year}')\n",
    "        url_gpkg = os.path.join(extract_path, f'cbs_vk100_{year}_v1.gpkg')\n",
    "\n",
    "        url = next((u for u in urls if str(year) in u), None)\n",
    "        if url:\n",
    "            # Check if the file already exists\n",
    "            if not os.path.exists(download_path):\n",
    "                if download_and_extract(url, year, download_path, extract_path):\n",
    "                    clip_gpkg(url_gpkg, '../shapefiles/zh_poly.shp', './clipped.geojson')\n",
    "            process_geojson('./clipped.geojson', 9, output_dir)\n",
    "            break\n",
    "        else:\n",
    "            print(f\"No URL available for {year}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
